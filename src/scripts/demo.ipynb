{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xlstm.xlstm_large.model import xLSTMLargeConfig, xLSTMLarge\n",
    "from mlstm_kernels.torch import get_available_mlstm_step_kernels, get_available_mlstm_kernels, get_available_mlstm_sequence_kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['chunkwise--native_autograd',\n",
       "  'chunkwise--native_custbw',\n",
       "  'chunkwise--triton_limit_chunk',\n",
       "  'chunkwise--triton_xl_chunk',\n",
       "  'chunkwise--triton_xl_chunk_siging',\n",
       "  'parallel--native_autograd',\n",
       "  'parallel--native_custbw',\n",
       "  'parallel--native_stablef_autograd',\n",
       "  'parallel--native_stablef_custbw',\n",
       "  'parallel--triton_limit_headdim',\n",
       "  'parallel--native_siging_autograd',\n",
       "  'parallel--native_siging_custbw'],\n",
       " ['native', 'triton'],\n",
       " ['native_sequence__native', 'native_sequence__triton'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_mlstm_kernels(), get_available_mlstm_step_kernels(), get_available_mlstm_sequence_kernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlstm_config = xLSTMLargeConfig(\n",
    "    embedding_dim=2048,\n",
    "    num_heads=8,\n",
    "    num_blocks=32,\n",
    "    vocab_size=65536,\n",
    "    return_last_states=True,\n",
    "    mode=\"inference\",\n",
    "    chunkwise_kernel=\"chunkwise--triton_xl_chunk\", # xl_chunk == TFLA kernels\n",
    "    sequence_kernel=\"native_sequence__triton\",\n",
    "    step_kernel=\"triton\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlstm = xLSTMLarge(xlstm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTMLarge(\n",
       "  (embedding): Embedding(65536, 2048)\n",
       "  (backbone): xLSTMLargeBlockStack(\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x mLSTMBlock(\n",
       "        (norm_mlstm): RMSNorm()\n",
       "        (mlstm_layer): mLSTMLayer(\n",
       "          (q): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (k): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (ogate_preact): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (igate_preact): Linear(in_features=2048, out_features=8, bias=True)\n",
       "          (fgate_preact): Linear(in_features=2048, out_features=8, bias=True)\n",
       "          (ogate_act_fn): Sigmoid()\n",
       "          (mlstm_backend): mLSTMBackend(mLSTMBackendConfig(chunkwise_kernel='chunkwise--triton_xl_chunk', sequence_kernel='native_sequence__triton', step_kernel='triton', mode='inference', chunk_size=64, return_last_states=True, autocast_kernel_dtype='bfloat16', eps=1e-06, inference_state_dtype='float32', normalize_siging=True))\n",
       "          (multihead_norm): MultiHeadLayerNorm()\n",
       "          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (norm_ffn): RMSNorm()\n",
       "        (ffn): FeedForward(\n",
       "          (proj_up_gate): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (proj_up): Linear(in_features=2048, out_features=5504, bias=False)\n",
       "          (proj_down): Linear(in_features=5504, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=65536, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlstm = xlstm.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[model] parameters ≈ 1888.7M'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"[model] parameters ≈ {sum(p.numel() for p in xlstm.parameters()) / 1e6:.1f}M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randint(0, 2048, (3, 256)).to(\"cuda\")\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"/usr/local/cuda/bin\") and \"/usr/local/cuda/bin\" not in os.environ[\"PATH\"]:\n",
    "    os.environ[\"PATH\"] += \":/usr/local/cuda/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = xlstm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(out) == 2:\n",
    "    out, state = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape[1:] == (256, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state), len(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([3, 256]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[:, 0:1].shape, input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_out, step_state = xlstm(input[:, 0:1], state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 65536])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_chunkwise, last_state_chunkwise = xlstm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.43 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m state = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28minput\u001b[39m.shape[\u001b[32m1\u001b[39m]):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     out_step, state = \u001b[43mxlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     out_steps.append(out_step)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/xlstm/xlstm_large/model.py:146\u001b[39m, in \u001b[36mxLSTMLarge.forward\u001b[39m\u001b[34m(self, x, state)\u001b[39m\n\u001b[32m    142\u001b[39m B, S = x.shape\n\u001b[32m    144\u001b[39m x = \u001b[38;5;28mself\u001b[39m.embedding(x)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m x, state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n\u001b[32m    149\u001b[39m logits_capped = soft_cap(logits, \u001b[38;5;28mself\u001b[39m.config.output_logit_soft_cap)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/xlstm/xlstm_large/model.py:217\u001b[39m, in \u001b[36mxLSTMLargeBlockStack.forward\u001b[39m\u001b[34m(self, x, state)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.blocks):\n\u001b[32m    216\u001b[39m     block_state = state[i]\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     x, block_state_new = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m block_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    220\u001b[39m         state[i] = block_state_new\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/xlstm/xlstm_large/model.py:503\u001b[39m, in \u001b[36mmLSTMBlock.forward\u001b[39m\u001b[34m(self, x, state)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    500\u001b[39m     \u001b[38;5;28mself\u001b[39m, x: torch.Tensor, state: mLSTMStateType | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    501\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, mLSTMStateType]:\n\u001b[32m    502\u001b[39m     x_mlstm = \u001b[38;5;28mself\u001b[39m.norm_mlstm(x)\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     x_mlstm, state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlstm_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_mlstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m     x = x + x_mlstm\n\u001b[32m    506\u001b[39m     x_ffn = \u001b[38;5;28mself\u001b[39m.norm_ffn(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/xlstm/xlstm_large/model.py:390\u001b[39m, in \u001b[36mmLSTMLayer.forward\u001b[39m\u001b[34m(self, x, state)\u001b[39m\n\u001b[32m    387\u001b[39m     v = \u001b[38;5;28mself\u001b[39m.v(x)\n\u001b[32m    388\u001b[39m     o_preact = \u001b[38;5;28mself\u001b[39m.ogate_preact(x)\n\u001b[32m    389\u001b[39m     i_preact = soft_cap(\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43migate_preact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, cap_value=\u001b[38;5;28mself\u001b[39m.config.gate_soft_cap\n\u001b[32m    391\u001b[39m     )\n\u001b[32m    392\u001b[39m     f_preact = soft_cap(\n\u001b[32m    393\u001b[39m         \u001b[38;5;28mself\u001b[39m.fgate_preact(x), cap_value=\u001b[38;5;28mself\u001b[39m.config.gate_soft_cap\n\u001b[32m    394\u001b[39m     )\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.weight_mode == \u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/OLMo-core/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.43 GiB of which 2.06 MiB is free. Including non-PyTorch memory, this process has 23.42 GiB memory in use. Of the allocated memory 21.88 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "out_steps = []\n",
    "state = None\n",
    "for i in range(input.shape[1]):\n",
    "    out_step, state = xlstm(input[:, i:i + 1], state)\n",
    "    out_steps.append(out_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_steps = torch.cat(out_steps, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 256, 2048]), torch.Size([3, 256, 2048]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_steps.shape, out_chunkwise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0138, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out_chunkwise - out_steps).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(out_chunkwise, out_steps, atol=7e-2, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embedding.weight',\n",
       " 'backbone.blocks.0.norm_mlstm.weight',\n",
       " 'backbone.blocks.0.mlstm_layer.q.weight',\n",
       " 'backbone.blocks.0.mlstm_layer.k.weight',\n",
       " 'backbone.blocks.0.mlstm_layer.v.weight',\n",
       " 'backbone.blocks.0.mlstm_layer.ogate_preact.weight',\n",
       " 'backbone.blocks.0.mlstm_layer.igate_preact.weight',\n",
       " 'backbone.blocks.0.mlstm_layer.igate_preact.bias',\n",
       " 'backbone.blocks.0.mlstm_layer.fgate_preact.weight',\n",
       " 'backbone.blocks.0.mlstm_layer.fgate_preact.bias',\n",
       " 'backbone.blocks.0.mlstm_layer.multihead_norm.weight',\n",
       " 'backbone.blocks.0.mlstm_layer.out_proj.weight',\n",
       " 'backbone.blocks.0.norm_ffn.weight',\n",
       " 'backbone.blocks.0.ffn.proj_up_gate.weight',\n",
       " 'backbone.blocks.0.ffn.proj_up.weight',\n",
       " 'backbone.blocks.0.ffn.proj_down.weight',\n",
       " 'backbone.blocks.1.norm_mlstm.weight',\n",
       " 'backbone.blocks.1.mlstm_layer.q.weight',\n",
       " 'backbone.blocks.1.mlstm_layer.k.weight',\n",
       " 'backbone.blocks.1.mlstm_layer.v.weight',\n",
       " 'backbone.blocks.1.mlstm_layer.ogate_preact.weight',\n",
       " 'backbone.blocks.1.mlstm_layer.igate_preact.weight',\n",
       " 'backbone.blocks.1.mlstm_layer.igate_preact.bias',\n",
       " 'backbone.blocks.1.mlstm_layer.fgate_preact.weight',\n",
       " 'backbone.blocks.1.mlstm_layer.fgate_preact.bias',\n",
       " 'backbone.blocks.1.mlstm_layer.multihead_norm.weight',\n",
       " 'backbone.blocks.1.mlstm_layer.out_proj.weight',\n",
       " 'backbone.blocks.1.norm_ffn.weight',\n",
       " 'backbone.blocks.1.ffn.proj_up_gate.weight',\n",
       " 'backbone.blocks.1.ffn.proj_up.weight',\n",
       " 'backbone.blocks.1.ffn.proj_down.weight',\n",
       " 'backbone.blocks.2.norm_mlstm.weight',\n",
       " 'backbone.blocks.2.mlstm_layer.q.weight',\n",
       " 'backbone.blocks.2.mlstm_layer.k.weight',\n",
       " 'backbone.blocks.2.mlstm_layer.v.weight',\n",
       " 'backbone.blocks.2.mlstm_layer.ogate_preact.weight',\n",
       " 'backbone.blocks.2.mlstm_layer.igate_preact.weight',\n",
       " 'backbone.blocks.2.mlstm_layer.igate_preact.bias',\n",
       " 'backbone.blocks.2.mlstm_layer.fgate_preact.weight',\n",
       " 'backbone.blocks.2.mlstm_layer.fgate_preact.bias',\n",
       " 'backbone.blocks.2.mlstm_layer.multihead_norm.weight',\n",
       " 'backbone.blocks.2.mlstm_layer.out_proj.weight',\n",
       " 'backbone.blocks.2.norm_ffn.weight',\n",
       " 'backbone.blocks.2.ffn.proj_up_gate.weight',\n",
       " 'backbone.blocks.2.ffn.proj_up.weight',\n",
       " 'backbone.blocks.2.ffn.proj_down.weight',\n",
       " 'backbone.blocks.3.norm_mlstm.weight',\n",
       " 'backbone.blocks.3.mlstm_layer.q.weight',\n",
       " 'backbone.blocks.3.mlstm_layer.k.weight',\n",
       " 'backbone.blocks.3.mlstm_layer.v.weight',\n",
       " 'backbone.blocks.3.mlstm_layer.ogate_preact.weight',\n",
       " 'backbone.blocks.3.mlstm_layer.igate_preact.weight',\n",
       " 'backbone.blocks.3.mlstm_layer.igate_preact.bias',\n",
       " 'backbone.blocks.3.mlstm_layer.fgate_preact.weight',\n",
       " 'backbone.blocks.3.mlstm_layer.fgate_preact.bias',\n",
       " 'backbone.blocks.3.mlstm_layer.multihead_norm.weight',\n",
       " 'backbone.blocks.3.mlstm_layer.out_proj.weight',\n",
       " 'backbone.blocks.3.norm_ffn.weight',\n",
       " 'backbone.blocks.3.ffn.proj_up_gate.weight',\n",
       " 'backbone.blocks.3.ffn.proj_up.weight',\n",
       " 'backbone.blocks.3.ffn.proj_down.weight',\n",
       " 'backbone.blocks.4.norm_mlstm.weight',\n",
       " 'backbone.blocks.4.mlstm_layer.q.weight',\n",
       " 'backbone.blocks.4.mlstm_layer.k.weight',\n",
       " 'backbone.blocks.4.mlstm_layer.v.weight',\n",
       " 'backbone.blocks.4.mlstm_layer.ogate_preact.weight',\n",
       " 'backbone.blocks.4.mlstm_layer.igate_preact.weight',\n",
       " 'backbone.blocks.4.mlstm_layer.igate_preact.bias',\n",
       " 'backbone.blocks.4.mlstm_layer.fgate_preact.weight',\n",
       " 'backbone.blocks.4.mlstm_layer.fgate_preact.bias',\n",
       " 'backbone.blocks.4.mlstm_layer.multihead_norm.weight',\n",
       " 'backbone.blocks.4.mlstm_layer.out_proj.weight',\n",
       " 'backbone.blocks.4.norm_ffn.weight',\n",
       " 'backbone.blocks.4.ffn.proj_up_gate.weight',\n",
       " 'backbone.blocks.4.ffn.proj_up.weight',\n",
       " 'backbone.blocks.4.ffn.proj_down.weight',\n",
       " 'backbone.blocks.5.norm_mlstm.weight',\n",
       " 'backbone.blocks.5.mlstm_layer.q.weight',\n",
       " 'backbone.blocks.5.mlstm_layer.k.weight',\n",
       " 'backbone.blocks.5.mlstm_layer.v.weight',\n",
       " 'backbone.blocks.5.mlstm_layer.ogate_preact.weight',\n",
       " 'backbone.blocks.5.mlstm_layer.igate_preact.weight',\n",
       " 'backbone.blocks.5.mlstm_layer.igate_preact.bias',\n",
       " 'backbone.blocks.5.mlstm_layer.fgate_preact.weight',\n",
       " 'backbone.blocks.5.mlstm_layer.fgate_preact.bias',\n",
       " 'backbone.blocks.5.mlstm_layer.multihead_norm.weight',\n",
       " 'backbone.blocks.5.mlstm_layer.out_proj.weight',\n",
       " 'backbone.blocks.5.norm_ffn.weight',\n",
       " 'backbone.blocks.5.ffn.proj_up_gate.weight',\n",
       " 'backbone.blocks.5.ffn.proj_up.weight',\n",
       " 'backbone.blocks.5.ffn.proj_down.weight',\n",
       " 'backbone.out_norm.weight',\n",
       " 'lm_head.weight']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(xlstm.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
